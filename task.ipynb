{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to identify and extract relevant words and phrases related to skills. This can be viewed as entity recognition task. \n",
    "The approach provided here uses spaCy NLP framework and make use most of its functionality. \n",
    "In general there are two possible approaches to this problem: rule-based approach or training a statistical entity recognition model.\n",
    "Rule-based approach is more practical if there is small value of training data and we have more or less finite number of examples which we want to find in the data.\n",
    "If we have enough of training data and want the system to be able to generalize based on these examples using local context, then we can use model training approach. \n",
    "In this task I have implemented both models. \n",
    "\n",
    "\n",
    "The version given below shows implementation of the rule-based approach. It is assume that we specify a dictionary of phrases for particular skill types as a pattern to be search in the text. For example,skill_dic = {\"hard skill\" : ['Python', 'Machine Learning'], \"soft skill\" : ['communication','team player']}  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import csv as csv\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data preprocessing pipline includes the function 'textToTokens()' which transforms a raw text to a list of tokens. Each token is extracted from the text and represents normolized version of the text words. It utilizes helper function 'isTokenValid()' which filter out punctuation symbol and stop-words. We also perform lemmatization and lowering the case of letters. As a result, the following is performed:\n",
    " - Lowercases the text\n",
    " - Lemmatizes each token\n",
    " - Removes punctuation symbols\n",
    " - Removes stop words  \n",
    "We also have a helper function  'listToString()' which returns wite space separated text string from the token list. Basically, by applying this fucnction to the preprocessed token list we can obtain normolized version of the original text  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(tokens):\n",
    "    return str(' '.join(tokens))\n",
    "\n",
    "\n",
    "def isTokenValid(token):\n",
    "    return bool(\n",
    "        token\n",
    "        and str(token).strip()\n",
    "        and not token.is_stop\n",
    "        and not token.is_punct\n",
    "    )\n",
    "\n",
    "\n",
    "def preprocessToken(token):\n",
    "    return token.lemma_.strip().lower()\n",
    "\n",
    "\n",
    "def textToTokens(nlp,raw_text):\n",
    "    nlp_doc = nlp(raw_text)\n",
    "    filtered_tokens = [ preprocessToken(token) for token in nlp_doc if isTokenValid(token)]\n",
    "    return filtered_tokens"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the Model class implements main task of finding relevant skill phrases. This class is initialized with nlp model. Function 'train()'  performs actual phrases matcher configuration based on the skills dictionary, and the function 'getSkills()' serchs across the text and output result variable 'result_match' with corresponding found skills phrases with corresponding labels. In other words, 'result_match' has the same structure of dictionary as 'skill_dic_processed' but with entries found in the text.\n",
    "The algorithm utilizes  'PhraseMatcher' class which allows specifying patterns along with label (or key) which identifies patterns. In our case, patterns are skill phrases and label is a skill type.\n",
    "Function 'getSkills()' returns a dictionary of the form: {sklill type: set(skill phrases)} Using set() structure allows us to eliminate duplocation in the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self,nlp):\n",
    "        self.nlp=nlp\n",
    "        #initilize the matcher with a shared vocab\n",
    "        self.matcher = PhraseMatcher(self.nlp.vocab)\n",
    "\n",
    "    def train(self,skill_dic_processed):\n",
    "        #add the pattern to the matcher\n",
    "        for key in skill_dic_processed:\n",
    "            patterns = [self.nlp(skill) for skill in skill_dic_processed[key]]\n",
    "            self.matcher.add(key, patterns) \n",
    "    #Find matches\n",
    "    def getSkills(self,text_processed):\n",
    "        result_match = {}       \n",
    "        doc = self.nlp(text_processed)\n",
    "        matches = self.matcher(doc)\n",
    "        for match_id, start, end in matches:\n",
    "            key = self.nlp.vocab.strings[match_id]\n",
    "            span = doc[start:end]\n",
    "            if key not in result_match: result_match[key]=set()\n",
    "            result_match[key].add(span.text)\n",
    "        return result_match\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of running the skill searching model. Here, we load an NLP model and specify dictionaty of skills. \n",
    "Source file is situated in the working directory and has at least two fields: ['id','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hard skill': {'sql', 'machine learning', 'python'}, 'soft skill': {'team player'}}\n"
     ]
    }
   ],
   "source": [
    "if __name__==\"__main__\":\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    skill_dic = {\"hard skill\" : ['Python', 'Java', 'C++', 'Machine Learning', 'Data Analysis', 'SQL'],\n",
    "                  \"soft skill\" : ['communication','leadership','team player']}\n",
    "    skill_dic_processed = {}\n",
    "    for key in skill_dic:\n",
    "        skill_dic_processed[key]=[listToString(textToTokens(nlp,skill)) for skill in skill_dic[key]]\n",
    "\n",
    "    \n",
    "    header=['id','text']\n",
    "    path=cwd = os.getcwd()+'\\\\cv_data.csv'\n",
    "\n",
    "    cv_data=[]\n",
    "    with open(path, 'r') as file:\n",
    "        csv_file = csv.DictReader(file)\n",
    "        for row in csv_file:\n",
    "            cv_data.append(dict(row))\n",
    "    \n",
    "\n",
    "    for data in cv_data:\n",
    "        text_processed = listToString(textToTokens(nlp,data['text'])) \n",
    "        skill_extractor=Model(nlp)\n",
    "        skill_extractor.train(skill_dic_processed)\n",
    "        print(skill_extractor.getSkills(text_processed))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, is another virsion which utilises training NLP model. It has dictionary of skills and a training dataset, which may contain phrases from the skill dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch\n",
    "import random\n",
    "from spacy.training.example import Example\n",
    "\n",
    "\n",
    "\n",
    "skill_dic = {\"hard skill\" : ['Python', 'Java', 'C++', 'Machine Learning', 'Data Analysis', 'SQL'],\n",
    "             \"soft skill\" : ['communication','leadership','team player']}\n",
    "\n",
    "train_data =[ \"Python is a programming language\",\n",
    "              \"Java is a programming language\",\n",
    "              \"C++ is a programming language\",\n",
    "              \"SQL is a query language\",\n",
    "              \"machine learning is an important skill\",\n",
    "           ]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model process training data by automatically finding skill keywords in the training data and labelling the sentances.\n",
    "Then it traines Named Entity Pipline of the NLP model with new skill entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelStatistic:\n",
    "    def __init__(self,nlp,):\n",
    "        self.nlp=nlp\n",
    "        self.labels=[]\n",
    "        self.result_match={}\n",
    "\n",
    "    def train(self,train_data_processed,skill_dic_processed):\n",
    "        self.labels = list(skill_dic_processed.keys())\n",
    "        # Getting the pipeline component\n",
    "        ner=self.nlp.get_pipe(\"ner\") \n",
    "        train_data = []\n",
    "        # Reformat training data sutable for spaCy training pipeline\n",
    "        for training_entry in train_data_processed:\n",
    "           for key in skill_dic_processed:\n",
    "              for entry in skill_dic_processed[key]:\n",
    "                 found = training_entry.find(entry)\n",
    "                 if found>=0:\n",
    "                    spacy_entry = (training_entry, {\"entities\": [(found,len(str(entry)),key)]})\n",
    "                    train_data.append(spacy_entry)\n",
    "        # Adding labels to the `ner`\n",
    "        for _, annotations in train_data:\n",
    "          for ent in annotations.get(\"entities\"):\n",
    "            ner.add_label(ent[2])\n",
    "        # List of pipes you want to train\n",
    "        pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "        unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "        # Begin training by disabling other pipeline components\n",
    "        with self.nlp.disable_pipes(*unaffected_pipes) :\n",
    "          # Training for 100 iterations     \n",
    "          for itn in range(100):\n",
    "            # shuffle examples before training\n",
    "            random.shuffle(train_data)\n",
    "            # batch up the examples using spaCy's minibatch\n",
    "            batches = minibatch(train_data, size=2)\n",
    "            for batch in batches:\n",
    "              texts, annotations = zip(*batch)\n",
    "              example = []\n",
    "            # Update the model with iterating each text\n",
    "            for i in range(len(texts)):\n",
    "                doc = self.nlp.make_doc(texts[i])\n",
    "                example.append(Example.from_dict(doc, annotations[i]))\n",
    "            self.nlp.update(example)\n",
    "\n",
    "    def getSkills(self,test_text_preprocessed):\n",
    "        doc = self.nlp(test_text_preprocessed)\n",
    "        for ent in doc.ents:\n",
    "            key = self.nlp.vocab.strings[ent.label]\n",
    "            if key in self.labels:\n",
    "              if key not in self.result_match: self.result_match[key]=set()\n",
    "              self.result_match[key].add(ent.text)\n",
    "        return self.result_match\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of using the model is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hard skill': {'python'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "nlp=spacy.load(\"en_core_web_sm\")\n",
    "skill_dic_processed = {}\n",
    "for key in skill_dic:\n",
    "    skill_dic_processed[key]=[listToString(textToTokens(nlp,skill)) for skill in skill_dic[key]]\n",
    "\n",
    "train_data_processed=[listToString(textToTokens(nlp,skill)) for skill in train_data]\n",
    "\n",
    "model_statistic = ModelStatistic(nlp)\n",
    "model_statistic.train(train_data_processed,skill_dic_processed)\n",
    "\n",
    "header=['id','text']\n",
    "path=cwd = os.getcwd()+'\\\\cv_data.csv'\n",
    "cv_data=[]\n",
    "with open(path, 'r') as file:\n",
    "    csv_file = csv.DictReader(file)\n",
    "    for row in csv_file:\n",
    "        cv_data.append(dict(row))\n",
    "\n",
    "for data in cv_data:\n",
    "    text_processed = listToString(textToTokens(nlp,data['text'])) \n",
    "    print(model_statistic.getSkills(text_processed))\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA SCIENCE QUESTIONS\n",
    "1. What is cross-validation, and why is it important in machine learning?\n",
    "    - Cross-validation is a technique to evaluate performance of the model. The data is split into training and validatin parts. After running training cycle, the model performance is validated on the validating data. It allows control overfitting, when the model performs well during training but could show bad results on during validation \n",
    "2. Can you explain the bias-variance trade-off in machine learning, and how it affects model\n",
    "performance?\n",
    "     - The bias-variance trade-off is a concept that refers to the trade-off between a model's ability to fit the training data (bias) and its ability to generalize to new, unseen data (variance). A model with high bias may be too simple and not capture all the relevant patterns in the data, resulting in underfitting. On the other hand, a model with high variance may be too complex and fit the noise in the data as well as the underlying patterns, resulting in overfitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SECURITY QUESTIONS\n",
    "1. Name the top 3 security risks you see to Fuel50's machine learning infrastructure and\n",
    "product at this point.\n",
    "     - Data breaches when machine learning models often uses sensitive data such as personal information, financial data, or trade secrets.\n",
    "     - Attackers manipulate the input data to cause the model to make incorrect predictions\n",
    "     - Employees or contractors with access to the machine learning infrastructure may intentionally or unintentionally misuse the system or leak sensitive data.\n",
    "\n",
    "3. Explain how you would monitor the performance and health of the deployed machine\n",
    "learning model in production environments.\n",
    " - Set up metrics for measuring model performance, such as accuracy, precision, recall, or F1-score.\n",
    " - Set up monitoring tools such as log analysis or anomaly detection .\n",
    " - Regularly audit the data used to train the model to ensure that it remains representative and unbiased.\n",
    "- Implement version control and rollback procedures to quickly address any issues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spaCy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
